{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MWI-Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R6DMvR8TzJiK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3GYzNWONtju",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning - Hw 2\n",
        "\n",
        "# Multi-class Weather Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDEJU1RwoIlv",
        "colab_type": "text"
      },
      "source": [
        "##Import \n",
        "\n",
        "Import libraries and print some versions.\n",
        "\n",
        "To use GPU, set `Edit / Notebook settings / Hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSKuPnIXNoq2",
        "colab_type": "code",
        "outputId": "869534c8-4609-4294-e359-a2af0dafa729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import warnings\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
        "                         Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\\\n",
        "                         UpSampling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Tensorflow version %s\" %tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.15.0\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBTpkAhdDIAs"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Get data from  https://drive.google.com/drive/folders/1UzH28Q8xki8_DMYdDgHxi40-CJ800Kaq\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8cdfb45c-6996-4c23-b506-8f47f344ed60",
        "id": "TgXrhbz6DIAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "datadir = '/content/drive/My Drive/Hw2-ML'\n",
        "trainingset = datadir + '/MWI-Dataset-1.1_3200/'\n",
        "testset = datadir + '/MWI-testset/'\n",
        "\n",
        "batch_size = 32\n",
        "input_shape = ()\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1. / 255,\\\n",
        "    zoom_range=0.1,\\\n",
        "    rotation_range=45,\\\n",
        "    width_shift_range=0.1,\\\n",
        "    height_shift_range=0.1,\\\n",
        "    horizontal_flip=True,\\\n",
        "    vertical_flip=False)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=trainingset,\n",
        "    target_size=(200, 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1. / 255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=testset,\n",
        "    target_size=(200, 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "num_samples = train_generator.n\n",
        "num_classes = train_generator.num_classes\n",
        "input_shape = train_generator.image_shape\n",
        "\n",
        "classnames = [k for k,v in train_generator.class_indices.items()]\n",
        "\n",
        "print(\"Image input %s\" %str(input_shape))\n",
        "print(\"Classes: %r\" %classnames)\n",
        "\n",
        "print('Loaded %d training samples from %d classes.' %(num_samples,num_classes))\n",
        "print('Loaded %d test samples from %d classes.' %(test_generator.n,test_generator.num_classes))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 3200 images belonging to 4 classes.\n",
            "Found 800 images belonging to 4 classes.\n",
            "Image input (200, 200, 3)\n",
            "Classes: ['HAZE', 'RAINY', 'SNOWY', 'SUNNY']\n",
            "Loaded 3200 training samples from 4 classes.\n",
            "Loaded 800 test samples from 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6DMvR8TzJiK",
        "colab_type": "text"
      },
      "source": [
        "## Show random image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0N3cUSazRcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 3\n",
        "x,y = train_generator.next()\n",
        "# x,y size is train_generator.batch_size\n",
        "\n",
        "for i in range(0,n):\n",
        "    image = x[i]\n",
        "    label = y[i].argmax()  # categorical from one-hot-encoding\n",
        "    print(classnames[label])\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN9_y7Ffa6Uy",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wumwGcGa9S3",
        "colab_type": "code",
        "outputId": "a9d54209-9db1-4d92-e3eb-768e35efc6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "from keras.models import load_model\n",
        "\n",
        "models_dir = datadir + '/models/'\n",
        "\n",
        "ValerioNet1 = 'MWI_ValerioNet1'\n",
        "ValerioNet2 = 'MWI_ValerioNet2'\n",
        "Tansfer_ValerioNet1 = 'MWI_ValerioNet1_Transfer'\n",
        "Tansfer_ValerioNet2 = 'MWI_ValerioNet2_Transfer'\n",
        "\n",
        "def loadmodel(problem):\n",
        "    filename = os.path.join(models_dir, '%s.h5' %problem)\n",
        "    try:\n",
        "        model = load_model(filename)\n",
        "        print(\"\\nModel loaded successfully from file %s\\n\" %filename)\n",
        "    except OSError:    \n",
        "        print(\"\\nModel file %s not found!!!\\n\" %filename)\n",
        "        model = None\n",
        "    return model\n",
        "\n",
        "def loadtransfermodel(problem):\n",
        "    filename = os.path.join(models_dir, '%s.h5' %problem)\n",
        "    try:\n",
        "        transfer_model = load_model(filename)\n",
        "        print(\"\\nModel loaded successfully from file %s\\n\" %filename)\n",
        "    except OSError:    \n",
        "        print(\"\\nModel file %s not found!!!\\n\" %filename)\n",
        "        transfer_model = None\n",
        "    return transfer_model\n",
        "\n",
        "model = loadmodel(ValerioNet2)\n",
        "#transfer_model = loadtransfermodel(Tansfer_ValerioNet1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model loaded successfully from file /content/drive/My Drive/Hw2-ML/models/MWI_ValerioNet2.h5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AnYJH-9YjCx2"
      },
      "source": [
        "##Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrxPaAovtV3z",
        "colab_type": "code",
        "outputId": "80924105-2557-440d-83dd-22635cc9ff5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import applications\n",
        "from keras.models import Model, Input\n",
        "\n",
        "\n",
        "def load_backbone_net(input_shape):\n",
        "    \n",
        "    # define input tensor\n",
        "    input0 = Input(shape=input_shape)\n",
        "\n",
        "    # load a pretrained model on imagenet without the final dense layer\n",
        "    feature_extractor = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=input0)\n",
        "    \n",
        "    \n",
        "    feature_extractor = feature_extractor.output\n",
        "    feature_extractor = Model(input=input0, output=feature_extractor)\n",
        "    optimizer = 'adam' #alternative 'SGD'\n",
        "\n",
        "    feature_extractor.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return feature_extractor\n",
        "\n",
        "\n",
        "def transferNet(feature_extractor, num_classes, output_layer_name, trainable_layers):\n",
        "    \n",
        "    # get the original input layer tensor\n",
        "    input_t = feature_extractor.get_layer(index=0).input\n",
        "\n",
        "    # set the feture extractor layers as non-trainable\n",
        "    for idx,layer in enumerate(feature_extractor.layers):\n",
        "      if layer.name in trainable_layers:\n",
        "        layer.trainable = True\n",
        "      else:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # get the output tensor from a layer of the feature extractor\n",
        "    output_extractor = feature_extractor.get_layer(name = output_layer_name).output\n",
        "    \n",
        "    #output_extractor = MaxPooling2D(pool_size=(4,4))(output_extractor)\n",
        "\n",
        "    # flat the output of a Conv layer\n",
        "    flatten = Flatten()(output_extractor) \n",
        "    flatten_norm = BatchNormalization()(flatten)\n",
        "\n",
        "    # add a Dense layer\n",
        "    dense = Dropout(0.4)(flatten_norm)\n",
        "    dense = Dense(200, activation='relu')(dense)\n",
        "    dense = BatchNormalization()(dense)\n",
        "    \n",
        "    # add a Dense layer\n",
        "    dense = Dropout(0.4)(dense)\n",
        "    dense = Dense(100, activation='relu')(dense)\n",
        "    dense = BatchNormalization()(dense)\n",
        "\n",
        "    # add the final output layer\n",
        "    dense = BatchNormalization()(dense)\n",
        "    dense = Dense(num_classes, activation='softmax')(dense)\n",
        "    \n",
        "\n",
        "    model = Model(input=input_t, output=dense, name=\"transferNet\")\n",
        "    \n",
        "    optimizer = 'adam' #alternative 'SGD'\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# load the pre-trained model\n",
        "feature_extractor = load_backbone_net(input_shape)\n",
        "feature_extractor.summary()\n",
        "\n",
        "\n",
        "# choose the layer from which you can get the features (block5_pool the end, glob_pooling to get the pooled version of the output)\n",
        "name_output_extractor = \"block5_pool\"\n",
        "trainable_layers = [\"block5_conv3\"]\n",
        "\n",
        "# build the transfer model\n",
        "transfer_model = transferNet(feature_extractor, num_classes, name_output_extractor, trainable_layers)\n",
        "transfer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 200, 200, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 200, 200, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 200, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 100, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 100, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 100, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 50, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 50, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 25, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 25, 25, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"transferNet\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 200, 200, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 200, 200, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 200, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 100, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 100, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 100, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 50, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 50, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 25, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 25, 25, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 18432)             73728     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               3686600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 18,497,120\n",
            "Trainable params: 6,104,576\n",
            "Non-trainable params: 12,392,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dtj4AAA3fA1",
        "colab_type": "code",
        "outputId": "55aab767-4e09-44d6-eed0-412ba8467e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the transferNet on the training data\n",
        "#stopping = callbacks.EarlyStopping(monitor='val_acc', patience=3)\n",
        "\n",
        "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
        "val_steps = test_generator.n//test_generator.batch_size+1\n",
        "\n",
        "try:\n",
        "    history_transfer = transfer_model.fit_generator(train_generator, epochs=50, verbose=1,\\\n",
        "                    steps_per_epoch=steps_per_epoch,\\\n",
        "                    validation_data=test_generator,\\\n",
        "                    validation_steps=val_steps)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 258s 3s/step - loss: 0.7120 - acc: 0.7309 - val_loss: 0.6337 - val_acc: 0.7837\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 71s 714ms/step - loss: 0.4676 - acc: 0.8219 - val_loss: 0.6979 - val_acc: 0.7560\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 73s 729ms/step - loss: 0.3958 - acc: 0.8494 - val_loss: 0.5677 - val_acc: 0.8101\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 0.3530 - acc: 0.8756 - val_loss: 0.7163 - val_acc: 0.7812\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 0.3106 - acc: 0.8859 - val_loss: 0.5623 - val_acc: 0.8041\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 73s 731ms/step - loss: 0.2945 - acc: 0.8884 - val_loss: 0.5936 - val_acc: 0.7945\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 73s 729ms/step - loss: 0.2896 - acc: 0.8841 - val_loss: 0.5382 - val_acc: 0.8125\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 73s 731ms/step - loss: 0.2977 - acc: 0.8903 - val_loss: 0.5614 - val_acc: 0.8149\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.2351 - acc: 0.9116 - val_loss: 0.5954 - val_acc: 0.8065\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 74s 736ms/step - loss: 0.2444 - acc: 0.9069 - val_loss: 0.6069 - val_acc: 0.8029\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 73s 732ms/step - loss: 0.2353 - acc: 0.9122 - val_loss: 0.6075 - val_acc: 0.8137\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.2083 - acc: 0.9225 - val_loss: 0.6399 - val_acc: 0.8173\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.2140 - acc: 0.9209 - val_loss: 0.7184 - val_acc: 0.7861\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 74s 740ms/step - loss: 0.2098 - acc: 0.9213 - val_loss: 0.6236 - val_acc: 0.8017\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1912 - acc: 0.9253 - val_loss: 0.8126 - val_acc: 0.7728\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1835 - acc: 0.9278 - val_loss: 0.7828 - val_acc: 0.7764\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.1785 - acc: 0.9266 - val_loss: 0.6276 - val_acc: 0.8161\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 74s 742ms/step - loss: 0.1919 - acc: 0.9269 - val_loss: 0.6165 - val_acc: 0.8161\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.1771 - acc: 0.9350 - val_loss: 0.5018 - val_acc: 0.8365\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1593 - acc: 0.9422 - val_loss: 0.6059 - val_acc: 0.8149\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1549 - acc: 0.9428 - val_loss: 0.5361 - val_acc: 0.8329\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 74s 735ms/step - loss: 0.1530 - acc: 0.9441 - val_loss: 0.7241 - val_acc: 0.8065\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1478 - acc: 0.9453 - val_loss: 0.6737 - val_acc: 0.8065\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 0.1435 - acc: 0.9481 - val_loss: 0.5856 - val_acc: 0.8221\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 74s 743ms/step - loss: 0.1494 - acc: 0.9456 - val_loss: 0.5640 - val_acc: 0.8341\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 72s 717ms/step - loss: 0.1367 - acc: 0.9488 - val_loss: 0.5846 - val_acc: 0.8257\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 73s 733ms/step - loss: 0.1553 - acc: 0.9431 - val_loss: 0.5467 - val_acc: 0.8377\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 0.1369 - acc: 0.9525 - val_loss: 0.5663 - val_acc: 0.8281\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 73s 730ms/step - loss: 0.1245 - acc: 0.9506 - val_loss: 0.8419 - val_acc: 0.7800\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 73s 729ms/step - loss: 0.1287 - acc: 0.9488 - val_loss: 0.7149 - val_acc: 0.7933\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 75s 747ms/step - loss: 0.1176 - acc: 0.9616 - val_loss: 0.6691 - val_acc: 0.8113\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.1029 - acc: 0.9659 - val_loss: 0.6622 - val_acc: 0.8293\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 73s 732ms/step - loss: 0.1095 - acc: 0.9625 - val_loss: 0.6992 - val_acc: 0.8101\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.1291 - acc: 0.9562 - val_loss: 0.6153 - val_acc: 0.8233\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 74s 736ms/step - loss: 0.1148 - acc: 0.9603 - val_loss: 0.5999 - val_acc: 0.8353\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 73s 734ms/step - loss: 0.0994 - acc: 0.9656 - val_loss: 0.6030 - val_acc: 0.8413\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 73s 731ms/step - loss: 0.0937 - acc: 0.9663 - val_loss: 0.6201 - val_acc: 0.8317\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.1126 - acc: 0.9600 - val_loss: 0.6889 - val_acc: 0.8293\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 74s 742ms/step - loss: 0.1115 - acc: 0.9562 - val_loss: 0.7582 - val_acc: 0.8185\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 74s 738ms/step - loss: 0.1285 - acc: 0.9553 - val_loss: 0.7319 - val_acc: 0.8149\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 0.1367 - acc: 0.9537 - val_loss: 0.7407 - val_acc: 0.8101\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 0.1034 - acc: 0.9641 - val_loss: 0.8279 - val_acc: 0.7969\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 74s 742ms/step - loss: 0.1186 - acc: 0.9606 - val_loss: 0.6447 - val_acc: 0.8293\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.1013 - acc: 0.9641 - val_loss: 0.6384 - val_acc: 0.8413\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.1063 - acc: 0.9637 - val_loss: 0.6072 - val_acc: 0.8438\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.0878 - acc: 0.9700 - val_loss: 0.6382 - val_acc: 0.8353\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 74s 736ms/step - loss: 0.0886 - acc: 0.9672 - val_loss: 0.6730 - val_acc: 0.8341\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 0.1183 - acc: 0.9628 - val_loss: 0.7295 - val_acc: 0.8173\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 74s 737ms/step - loss: 0.1005 - acc: 0.9647 - val_loss: 0.7879 - val_acc: 0.8113\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 74s 745ms/step - loss: 0.0886 - acc: 0.9688 - val_loss: 0.6718 - val_acc: 0.8329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2gcWDvD19N-",
        "colab_type": "text"
      },
      "source": [
        "## Save the transfer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JYK-2ES2AiA",
        "colab_type": "code",
        "outputId": "0fa50df5-3cc0-476d-8612-c44ad9880342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "\n",
        "models_dir = '/content/drive/My Drive/Hw2-ML/models/'\n",
        "\n",
        "def savemodel(model,problem):\n",
        "    filename = os.path.join(models_dir, '%s.h5' %problem)\n",
        "    transfer_model.save(filename)\n",
        "    print(\"\\nModel saved successfully on file %s\\n\" %filename)\n",
        "\n",
        "# Save the model\n",
        "savemodel(transfer_model,'MWI_ValerioNet2_Transfer')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model saved successfully on file /content/drive/My Drive/Hw2-ML/models/MWI_ValerioNet2_Transfer.h5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkx4x2M3vFpk",
        "colab_type": "text"
      },
      "source": [
        "## MWI-Dataset Evaluation Scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUE2GCMVRAnY",
        "colab_type": "text"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpusYHORDQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "63b7a189-9179-43ac-d659-cefce6370b9d"
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=testset,\n",
        "    target_size=(200, 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        ")\n",
        "val_steps=test_generator.n//test_generator.batch_size+1\n",
        "loss, acc = transfer_model.evaluate_generator(test_generator,verbose=1,steps=val_steps)\n",
        "print('Test loss: %f' %loss)\n",
        "print('Test accuracy: %f' %acc)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 4 classes.\n",
            "26/26 [==============================] - 9s 350ms/step\n",
            "Test loss: 0.658900\n",
            "Test accuracy: 0.835337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1pDmZkxizx8",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrix analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO7lVQiki3bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "56b66e24-daad-4762-d01e-4b8066eaa452"
      },
      "source": [
        "import sklearn.metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "val_steps=(test_generator.n//test_generator.batch_size+1)-1\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=testset,\n",
        "    target_size=(200, 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# predictions from transferNet\n",
        "preds = transfer_model.predict_generator(test_generator,verbose=1,steps=val_steps)\n",
        "\n",
        "Ypred = np.argmax(preds, axis=1)\n",
        "Ytest = test_generator.classes  # shuffle=False in test_generator\n",
        "\n",
        "cm = confusion_matrix(Ytest, Ypred)\n",
        "print(cm)\n",
        "\n",
        "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
        "for i in range(0,cm.shape[0]):\n",
        "  for j in range(0,cm.shape[1]):\n",
        "    if (i!=j and cm[i][j]>0):\n",
        "      conf.append([i,j,cm[i][j]])\n",
        "\n",
        "col=2\n",
        "conf = np.array(conf)\n",
        "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
        "\n",
        "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
        "print('------------------------------------------------------------------')\n",
        "for k in conf:\n",
        "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))\n",
        "  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 4 classes.\n",
            "25/25 [==============================] - 9s 372ms/step\n",
            "[[174   3   9  14]\n",
            " [  7 152  33   8]\n",
            " [  9  17 174   0]\n",
            " [ 12   9  13 166]]\n",
            "True                 Predicted         \terrors \terr % \n",
            "------------------------------------------------------------------\n",
            "RAINY            ->  SNOWY             \t33 \t4.12 % \n",
            "SNOWY            ->  RAINY             \t17 \t2.12 % \n",
            "HAZE             ->  SUNNY             \t14 \t1.75 % \n",
            "SUNNY            ->  SNOWY             \t13 \t1.62 % \n",
            "SUNNY            ->  HAZE              \t12 \t1.50 % \n",
            "HAZE             ->  SNOWY             \t9 \t1.12 % \n",
            "SNOWY            ->  HAZE              \t9 \t1.12 % \n",
            "SUNNY            ->  RAINY             \t9 \t1.12 % \n",
            "RAINY            ->  SUNNY             \t8 \t1.00 % \n",
            "RAINY            ->  HAZE              \t7 \t0.88 % \n",
            "HAZE             ->  RAINY             \t3 \t0.38 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpEaAyIy8YMZ",
        "colab_type": "text"
      },
      "source": [
        "Precision, Recall, F-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "869Q50i78Rqu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c98bd1da-efea-4ff2-9c9f-5fdb94cd32c0"
      },
      "source": [
        "import sklearn.metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "val_steps=(test_generator.n//test_generator.batch_size+1)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=testset,\n",
        "    target_size=(200 , 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "preds = transfer_model.predict_generator(test_generator,verbose=1,steps=val_steps-1)\n",
        "\n",
        "Ypred = np.argmax(preds, axis=1)\n",
        "Ytest = test_generator.classes  # shuffle=False in test_generator\n",
        "\n",
        "print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 4 classes.\n",
            "25/25 [==============================] - 9s 355ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        HAZE      0.861     0.870     0.866       200\n",
            "       RAINY      0.840     0.760     0.798       200\n",
            "       SNOWY      0.760     0.870     0.811       200\n",
            "       SUNNY      0.883     0.830     0.856       200\n",
            "\n",
            "    accuracy                          0.833       800\n",
            "   macro avg      0.836     0.833     0.833       800\n",
            "weighted avg      0.836     0.833     0.833       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_QHpBGltIDv",
        "colab_type": "text"
      },
      "source": [
        "## SMART-I Evaluation Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quDJrxNstS_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "1287c014-7791-420a-f755-b4ad3fdf73fa"
      },
      "source": [
        "import sklearn.metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "smarti_test = datadir + '/SMART-I_Dataset'\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=smarti_test,\n",
        "    target_size=(200, 200),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "val_steps=test_generator.n//test_generator.batch_size+1\n",
        "\n",
        "\n",
        "############## ACCURACY ##############\n",
        "loss, acc = transfer_model.evaluate_generator(test_generator,verbose=1,steps=val_steps)\n",
        "print('SMART-I loss: %f' %loss)\n",
        "print('SMART-I accuracy: %f' %acc)\n",
        "\n",
        "preds = transfer_model.predict_generator(test_generator,verbose=1,steps=val_steps)\n",
        "\n",
        "Ypred = np.argmax(preds, axis=1)\n",
        "Ytest = test_generator.classes  # shuffle=False in test_generator\n",
        "\n",
        "############## PRECISION, RECALL, F-SCORE ##############\n",
        "print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))\n",
        "\n",
        "############## CONFUSION MATRIX ##############\n",
        "\n",
        "cm = confusion_matrix(Ytest, Ypred)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
        "for i in range(0,cm.shape[0]):\n",
        "  for j in range(0,cm.shape[1]):\n",
        "    if (i!=j and cm[i][j]>0):\n",
        "      conf.append([i,j,cm[i][j]])\n",
        "\n",
        "col=2\n",
        "conf = np.array(conf)\n",
        "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
        "\n",
        "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
        "print('------------------------------------------------------------------')\n",
        "for k in conf:\n",
        "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3038 images belonging to 4 classes.\n",
            "95/95 [==============================] - 28s 295ms/step\n",
            "SMART-I loss: 4.196744\n",
            "SMART-I accuracy: 0.437788\n",
            "95/95 [==============================] - 27s 287ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        HAZE      0.000     0.000     0.000         0\n",
            "       RAINY      0.262     0.424     0.324       521\n",
            "       SNOWY      0.587     0.716     0.645      1421\n",
            "       SUNNY      0.708     0.084     0.150      1096\n",
            "\n",
            "    accuracy                          0.438      3038\n",
            "   macro avg      0.389     0.306     0.280      3038\n",
            "weighted avg      0.575     0.438     0.411      3038\n",
            "\n",
            "[[   0    0    0    0]\n",
            " [  83  221  208    9]\n",
            " [ 194  181 1017   29]\n",
            " [  54  442  508   92]]\n",
            "True                 Predicted         \terrors \terr % \n",
            "------------------------------------------------------------------\n",
            "SUNNY            ->  SNOWY             \t508 \t16.72 % \n",
            "SUNNY            ->  RAINY             \t442 \t14.55 % \n",
            "RAINY            ->  SNOWY             \t208 \t6.85 % \n",
            "SNOWY            ->  HAZE              \t194 \t6.39 % \n",
            "SNOWY            ->  RAINY             \t181 \t5.96 % \n",
            "RAINY            ->  HAZE              \t83 \t2.73 % \n",
            "SUNNY            ->  HAZE              \t54 \t1.78 % \n",
            "SNOWY            ->  SUNNY             \t29 \t0.95 % \n",
            "RAINY            ->  SUNNY             \t9 \t0.30 % \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}